"""ç•°å¸¸æ¤œçŸ¥ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆç®¡ç†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«"""
import json
import os
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import logging
from dataclasses import dataclass
from collections import deque

from config import settings
from src.notification.slack_notifier import SlackNotifier

logger = logging.getLogger(__name__)


@dataclass
class ExecutionResult:
    """å®Ÿè¡Œçµæœãƒ‡ãƒ¼ã‚¿"""
    timestamp: datetime
    success: bool
    articles_found: int
    articles_verified: int
    processing_time_seconds: float
    error_message: Optional[str] = None


@dataclass
class Alert:
    """ã‚¢ãƒ©ãƒ¼ãƒˆæƒ…å ±"""
    type: str  # "consecutive_failures", "low_articles", "performance_degradation"
    severity: str  # "warning", "critical"
    message: str
    details: Dict
    timestamp: datetime


class AnomalyDetector:
    """ç•°å¸¸æ¤œçŸ¥ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    EXECUTION_HISTORY_FILE = "execution_history.json"
    ALERT_HISTORY_FILE = "alert_history.json"
    
    # é–¾å€¤è¨­å®š
    CONSECUTIVE_FAILURE_THRESHOLD = 3  # é€£ç¶šå¤±æ•—å›æ•°
    MIN_ARTICLES_WARNING = 2  # è¨˜äº‹æ•°è­¦å‘Šé–¾å€¤
    MIN_ARTICLES_CRITICAL = 0  # è¨˜äº‹æ•°ç•°å¸¸é–¾å€¤
    PERFORMANCE_WARNING_FACTOR = 2.0  # å‡¦ç†æ™‚é–“è­¦å‘Šå€ç‡
    PERFORMANCE_CRITICAL_FACTOR = 3.0  # å‡¦ç†æ™‚é–“ç•°å¸¸å€ç‡
    
    def __init__(self):
        self.slack_notifier = SlackNotifier()
        self.execution_history = deque(maxlen=100)  # æœ€æ–°100ä»¶ã‚’ä¿æŒ
        self.alert_history = []
        self.baseline_performance = None
        
        self._load_history()
        self._calculate_baseline()
    
    def record_execution(self, result: ExecutionResult):
        """å®Ÿè¡Œçµæœã‚’è¨˜éŒ²"""
        self.execution_history.append({
            'timestamp': result.timestamp.isoformat(),
            'success': result.success,
            'articles_found': result.articles_found,
            'articles_verified': result.articles_verified,
            'processing_time_seconds': result.processing_time_seconds,
            'error_message': result.error_message
        })
        
        self._save_history()
        
        # ç•°å¸¸æ¤œçŸ¥å®Ÿè¡Œ
        alerts = self._detect_anomalies(result)
        
        # ã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡
        for alert in alerts:
            self._send_alert(alert)
    
    def _detect_anomalies(self, latest_result: ExecutionResult) -> List[Alert]:
        """ç•°å¸¸ã‚’æ¤œçŸ¥"""
        alerts = []
        
        # 1. é€£ç¶šå®Ÿè¡Œå¤±æ•—ãƒã‚§ãƒƒã‚¯
        failure_alert = self._check_consecutive_failures()
        if failure_alert:
            alerts.append(failure_alert)
        
        # 2. è¨˜äº‹æ•°ç•°å¸¸ãƒã‚§ãƒƒã‚¯
        if latest_result.success:
            article_alert = self._check_article_count(latest_result)
            if article_alert:
                alerts.append(article_alert)
        
        # 3. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŠ£åŒ–ãƒã‚§ãƒƒã‚¯
        if latest_result.success and self.baseline_performance:
            performance_alert = self._check_performance(latest_result)
            if performance_alert:
                alerts.append(performance_alert)
        
        return alerts
    
    def _check_consecutive_failures(self) -> Optional[Alert]:
        """é€£ç¶šå®Ÿè¡Œå¤±æ•—ã‚’ãƒã‚§ãƒƒã‚¯"""
        if len(self.execution_history) < self.CONSECUTIVE_FAILURE_THRESHOLD:
            return None
        
        # æœ€æ–°ã®Nä»¶ã‚’ãƒã‚§ãƒƒã‚¯
        recent_executions = list(self.execution_history)[-self.CONSECUTIVE_FAILURE_THRESHOLD:]
        all_failed = all(not exec_data['success'] for exec_data in recent_executions)
        
        if all_failed:
            error_messages = [
                exec_data['error_message'] 
                for exec_data in recent_executions 
                if exec_data['error_message']
            ]
            
            return Alert(
                type="consecutive_failures",
                severity="critical",
                message=f"ğŸš¨ {self.CONSECUTIVE_FAILURE_THRESHOLD}å›é€£ç¶šã§å®Ÿè¡Œã«å¤±æ•—ã—ã¦ã„ã¾ã™",
                details={
                    'failure_count': self.CONSECUTIVE_FAILURE_THRESHOLD,
                    'error_messages': error_messages
                },
                timestamp=datetime.now()
            )
        
        return None
    
    def _check_article_count(self, result: ExecutionResult) -> Optional[Alert]:
        """è¨˜äº‹æ•°ç•°å¸¸ã‚’ãƒã‚§ãƒƒã‚¯"""
        if result.articles_verified <= self.MIN_ARTICLES_CRITICAL:
            return Alert(
                type="low_articles",
                severity="critical",
                message=f"ğŸš¨ æ¤œè¨¼æ¸ˆã¿è¨˜äº‹ãŒ{result.articles_verified}ä»¶ã—ã‹ã‚ã‚Šã¾ã›ã‚“",
                details={
                    'articles_found': result.articles_found,
                    'articles_verified': result.articles_verified,
                    'expected_minimum': settings.MAX_ARTICLES_PER_DAY
                },
                timestamp=datetime.now()
            )
        elif result.articles_verified <= self.MIN_ARTICLES_WARNING:
            return Alert(
                type="low_articles",
                severity="warning",
                message=f"âš ï¸ æ¤œè¨¼æ¸ˆã¿è¨˜äº‹ãŒå°‘ãªããªã£ã¦ã„ã¾ã™ï¼ˆ{result.articles_verified}ä»¶ï¼‰",
                details={
                    'articles_found': result.articles_found,
                    'articles_verified': result.articles_verified,
                    'expected': settings.MAX_ARTICLES_PER_DAY
                },
                timestamp=datetime.now()
            )
        
        return None
    
    def _check_performance(self, result: ExecutionResult) -> Optional[Alert]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŠ£åŒ–ã‚’ãƒã‚§ãƒƒã‚¯"""
        if not self.baseline_performance:
            return None
        
        baseline_time = self.baseline_performance['avg_processing_time']
        current_time = result.processing_time_seconds
        
        if current_time > baseline_time * self.PERFORMANCE_CRITICAL_FACTOR:
            return Alert(
                type="performance_degradation",
                severity="critical",
                message=f"ğŸš¨ å‡¦ç†æ™‚é–“ãŒç•°å¸¸ã«é•·ããªã£ã¦ã„ã¾ã™ï¼ˆ{current_time:.1f}ç§’ï¼‰",
                details={
                    'current_time': current_time,
                    'baseline_time': baseline_time,
                    'factor': current_time / baseline_time
                },
                timestamp=datetime.now()
            )
        elif current_time > baseline_time * self.PERFORMANCE_WARNING_FACTOR:
            return Alert(
                type="performance_degradation",
                severity="warning",
                message=f"âš ï¸ å‡¦ç†æ™‚é–“ãŒå¢—åŠ ã—ã¦ã„ã¾ã™ï¼ˆ{current_time:.1f}ç§’ï¼‰",
                details={
                    'current_time': current_time,
                    'baseline_time': baseline_time,
                    'factor': current_time / baseline_time
                },
                timestamp=datetime.now()
            )
        
        return None
    
    def _send_alert(self, alert: Alert):
        """ã‚¢ãƒ©ãƒ¼ãƒˆã‚’é€ä¿¡"""
        # é‡è¤‡ã‚¢ãƒ©ãƒ¼ãƒˆé˜²æ­¢ï¼ˆåŒã˜ã‚¿ã‚¤ãƒ—ã®ã‚¢ãƒ©ãƒ¼ãƒˆã¯1æ™‚é–“ã«1å›ã¾ã§ï¼‰
        if self._is_duplicate_alert(alert):
            logger.info(f"é‡è¤‡ã‚¢ãƒ©ãƒ¼ãƒˆã®ãŸã‚é€ä¿¡ã‚’ã‚¹ã‚­ãƒƒãƒ—: {alert.type}")
            return
        
        # ã‚¢ãƒ©ãƒ¼ãƒˆå±¥æ­´ã«è¿½åŠ 
        self.alert_history.append({
            'type': alert.type,
            'severity': alert.severity,
            'message': alert.message,
            'details': alert.details,
            'timestamp': alert.timestamp.isoformat()
        })
        self._save_alert_history()
        
        # Slackç°¡æ˜“ãƒ†ã‚­ã‚¹ãƒˆé€šçŸ¥ã«ç§»è¡Œ
        title_emoji = "ğŸ”´" if alert.severity == "critical" else "âš ï¸"
        lines = [
            f"{title_emoji} AI News Feeder ã‚¢ãƒ©ãƒ¼ãƒˆ",
            f"ç¨®åˆ¥: {self._get_alert_type_name(alert.type)}",
            f"é‡è¦åº¦: {alert.severity.upper()}",
            alert.message,
        ]
        # è©³ç´°è¿½è¨˜
        if alert.type == "consecutive_failures" and alert.details.get('error_messages'):
            lines.append(f"æœ€æ–°ã‚¨ãƒ©ãƒ¼: {alert.details['error_messages'][-1][:200]}")
        elif alert.type == "low_articles":
            lines.append(
                f"è¨˜äº‹æ•°: æ¤œè¨¼æ¸ˆã¿ {alert.details['articles_verified']} / ç™ºè¦‹ {alert.details['articles_found']}"
            )
        elif alert.type == "performance_degradation":
            lines.append(
                f"å‡¦ç†æ™‚é–“: {alert.details['current_time']:.1f}s (é€šå¸¸ã®{alert.details['factor']:.1f}å€)"
            )
        text = "\n".join(lines)
        ok = self.slack_notifier.send_notification(text)
        if ok:
            logger.info(f"ã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡æˆåŠŸ: {alert.type}")
        else:
            logger.error("ã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡å¤±æ•—")
    
    def _is_duplicate_alert(self, alert: Alert) -> bool:
        """é‡è¤‡ã‚¢ãƒ©ãƒ¼ãƒˆã‹ãƒã‚§ãƒƒã‚¯ï¼ˆ1æ™‚é–“ä»¥å†…ã®åŒã˜ã‚¿ã‚¤ãƒ—ï¼‰"""
        one_hour_ago = datetime.now() - timedelta(hours=1)
        
        for past_alert in reversed(self.alert_history):
            alert_time = datetime.fromisoformat(past_alert['timestamp'])
            if alert_time < one_hour_ago:
                break
            
            if past_alert['type'] == alert.type and past_alert['severity'] == alert.severity:
                return True
        
        return False
    
    def _get_alert_type_name(self, alert_type: str) -> str:
        """ã‚¢ãƒ©ãƒ¼ãƒˆã‚¿ã‚¤ãƒ—ã®æ—¥æœ¬èªåã‚’å–å¾—"""
        names = {
            'consecutive_failures': 'é€£ç¶šå®Ÿè¡Œå¤±æ•—',
            'low_articles': 'è¨˜äº‹æ•°ä¸è¶³',
            'performance_degradation': 'ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŠ£åŒ–'
        }
        return names.get(alert_type, alert_type)
    
    def _calculate_baseline(self):
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ€§èƒ½ã‚’è¨ˆç®—"""
        if len(self.execution_history) < 10:
            return
        
        # æˆåŠŸã—ãŸå®Ÿè¡Œã®ã¿ã‚’å¯¾è±¡
        successful_executions = [
            exec_data for exec_data in self.execution_history
            if exec_data['success']
        ]
        
        if len(successful_executions) < 5:
            return
        
        # æœ€æ–°20ä»¶ã®å¹³å‡ã‚’è¨ˆç®—
        recent_successful = successful_executions[-20:]
        avg_time = sum(exec_data['processing_time_seconds'] for exec_data in recent_successful) / len(recent_successful)
        avg_articles = sum(exec_data['articles_verified'] for exec_data in recent_successful) / len(recent_successful)
        
        self.baseline_performance = {
            'avg_processing_time': avg_time,
            'avg_articles_verified': avg_articles,
            'sample_size': len(recent_successful)
        }
        
        logger.info(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è¨ˆç®—å®Œäº†: å¹³å‡å‡¦ç†æ™‚é–“={avg_time:.1f}ç§’, å¹³å‡è¨˜äº‹æ•°={avg_articles:.1f}")
    
    def _save_history(self):
        """å®Ÿè¡Œå±¥æ­´ã‚’ä¿å­˜"""
        try:
            with open(self.EXECUTION_HISTORY_FILE, 'w') as f:
                json.dump(list(self.execution_history), f, indent=2)
        except Exception as e:
            logger.error(f"å®Ÿè¡Œå±¥æ­´ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _save_alert_history(self):
        """ã‚¢ãƒ©ãƒ¼ãƒˆå±¥æ­´ã‚’ä¿å­˜"""
        try:
            # æœ€æ–°100ä»¶ã®ã¿ä¿æŒ
            self.alert_history = self.alert_history[-100:]
            
            with open(self.ALERT_HISTORY_FILE, 'w') as f:
                json.dump(self.alert_history, f, indent=2)
        except Exception as e:
            logger.error(f"ã‚¢ãƒ©ãƒ¼ãƒˆå±¥æ­´ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _load_history(self):
        """å±¥æ­´ã‚’èª­ã¿è¾¼ã¿"""
        # å®Ÿè¡Œå±¥æ­´
        if os.path.exists(self.EXECUTION_HISTORY_FILE):
            try:
                with open(self.EXECUTION_HISTORY_FILE, 'r') as f:
                    data = json.load(f)
                    self.execution_history = deque(data, maxlen=100)
            except Exception as e:
                logger.error(f"å®Ÿè¡Œå±¥æ­´èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        
        # ã‚¢ãƒ©ãƒ¼ãƒˆå±¥æ­´
        if os.path.exists(self.ALERT_HISTORY_FILE):
            try:
                with open(self.ALERT_HISTORY_FILE, 'r') as f:
                    self.alert_history = json.load(f)
            except Exception as e:
                logger.error(f"ã‚¢ãƒ©ãƒ¼ãƒˆå±¥æ­´èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    def get_recent_alerts(self, hours: int = 24) -> List[Dict]:
        """æœ€è¿‘ã®ã‚¢ãƒ©ãƒ¼ãƒˆã‚’å–å¾—"""
        cutoff_time = datetime.now() - timedelta(hours=hours)
        recent = []
        
        for alert in self.alert_history:
            alert_time = datetime.fromisoformat(alert['timestamp'])
            if alert_time >= cutoff_time:
                recent.append(alert)
        
        return recent
    
    def get_execution_stats(self) -> Dict:
        """å®Ÿè¡Œçµ±è¨ˆã‚’å–å¾—"""
        if not self.execution_history:
            return {}
        
        total = len(self.execution_history)
        successful = sum(1 for exec_data in self.execution_history if exec_data['success'])
        
        # 24æ™‚é–“ä»¥å†…ã®çµ±è¨ˆ
        day_ago = datetime.now() - timedelta(hours=24)
        recent_executions = [
            exec_data for exec_data in self.execution_history
            if datetime.fromisoformat(exec_data['timestamp']) >= day_ago
        ]
        
        recent_total = len(recent_executions)
        recent_successful = sum(1 for exec_data in recent_executions if exec_data['success'])
        
        return {
            'total_executions': total,
            'successful_executions': successful,
            'success_rate': (successful / total * 100) if total > 0 else 0,
            'recent_24h': {
                'total': recent_total,
                'successful': recent_successful,
                'success_rate': (recent_successful / recent_total * 100) if recent_total > 0 else 0
            },
            'baseline_performance': self.baseline_performance
        }
